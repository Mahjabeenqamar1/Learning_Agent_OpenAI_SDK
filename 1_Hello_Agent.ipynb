{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahjabeenqamar1/Learning_Agent_OpenAI_SDK/blob/main/1_Hello_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installation"
      ],
      "metadata": {
        "id": "HEbPNUzuEvTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04WYHNNY5Tj6",
        "outputId": "af53cfc4-28b4-4fd3-a060-b062d5c3747b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/237.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m235.5/237.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m237.6/237.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/150.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "gr5m0ol9FA8e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled\n"
      ],
      "metadata": {
        "id": "0GUItyurFA_2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Google Gemini with OPENAI-Agent SDK"
      ],
      "metadata": {
        "id": "AEouQhbAGO4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "from agents.run import RunConfig\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "YqPrEz0mFBDZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "#check if the API key is present; if not, raise an error\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY not set. Please add it to the userdata.env file.\")\n",
        "\n",
        "#Reference :https://ai.google.dev/gemini-api/docs/openai\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.5-flash\", # Changed from gemini-2.0-flash to gemini-2.5-flash\n",
        "    openai_client=external_client,\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model = model,\n",
        "    tracing_disabled=True, # Removed model_provider=external_client\n",
        ")"
      ],
      "metadata": {
        "id": "2H09NT-jFBHh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hello world code |method one"
      ],
      "metadata": {
        "id": "8loqP72QJT3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from google.colab import userdata\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "\n",
        "# Define gemini_api_key if not already defined globally\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY not set. Please add it to the userdata.env file.\")\n",
        "\n",
        "# Define external_client and llm_model if not already defined globally\n",
        "external_client: AsyncOpenAI = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "# Define the asynchronous function to run the agent\n",
        "async def run_hello_world_agent():\n",
        "    agent : Agent = Agent(name=\"Assistant\", instructions=\"you are a helpful assistant\", model=llm_model)\n",
        "\n",
        "    # Create a new RunConfig using the working llm_model\n",
        "    local_run_config = RunConfig(\n",
        "        model=llm_model,\n",
        "        tracing_disabled=True,\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(agent, \"What is the capital of France?\", run_config=local_run_config)\n",
        "    print(\"\\nCALLING AGENT\\n\")\n",
        "    print(result.final_output)\n",
        "\n",
        "# Run the asynchronous function\n",
        "await run_hello_world_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sRB35-BFBLL",
        "outputId": "e9d9b557-bf77-4eb6-96e6-22148b94ec44"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CALLING AGENT\n",
            "\n",
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracing disabled\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "# 1. Which LLM Service?\n",
        "external_client: AsyncOpenAI = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "# 2. Which LLM Model?\n",
        "llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    openai_client=external_client\n",
        ")"
      ],
      "metadata": {
        "id": "1NcPzdp1Paxd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Running Agent Synchronously"
      ],
      "metadata": {
        "id": "BtrGLGhLPKNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from google.colab import userdata\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "\n",
        "# Ensure gemini_api_key is defined for this cell's execution\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GEMINI_API_KEY not set. Please add it to the userdata.env file.\")\n",
        "\n",
        "# Ensure external_client and llm_model are defined for this cell's execution\n",
        "external_client: AsyncOpenAI = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "async def run_math_agent():\n",
        "    math_agent: Agent = Agent(name=\"MathAgent\",\n",
        "                         instructions=\"You are a helpful math assistant.\",\n",
        "                         model=llm_model) # gemini-2.5 as agent brain - chat completions\n",
        "\n",
        "    # Create a RunConfig for this specific run\n",
        "    local_run_config = RunConfig(\n",
        "        model=llm_model,\n",
        "        tracing_disabled=True,\n",
        "    )\n",
        "\n",
        "    # Use the asynchronous run method and await it\n",
        "    result = await Runner.run(math_agent, \"why learn math for AI Agents?\", run_config=local_run_config)\n",
        "\n",
        "    print(\"\\nCALLING AGENT\\n\")\n",
        "    print(result.final_output)\n",
        "\n",
        "# Run the asynchronous function\n",
        "await run_math_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2g2sJYTPSrj",
        "outputId": "9e7c2076-183f-45fc-d76a-a745898f3e5b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CALLING AGENT\n",
            "\n",
            "Learning mathematics is absolutely fundamental for anyone working with or designing AI agents, and it forms the bedrock upon which all advanced AI systems are built. Here's a breakdown of why:\n",
            "\n",
            "1.  **The Language of Data and Uncertainty (Statistics & Probability):**\n",
            "    *   **Understanding Data:** AI agents process vast amounts of data. Statistics provides tools to analyze, summarize, and understand the patterns, distributions, and relationships within that data.\n",
            "    *   **Dealing with Uncertainty:** The real world is inherently uncertain. Probability theory is essential for agents to quantify uncertainty, make predictions, and reason under incomplete or noisy information (e.g., Bayesian inference, Markov Decision Processes).\n",
            "    *   **Machine Learning Models:** Many machine learning algorithms (like Naive Bayes, Logistic Regression, Hidden Markov Models, even aspects of deep learning) are rooted in statistical and probabilistic concepts.\n",
            "\n",
            "2.  **The Engine of Learning and Transformation (Linear Algebra):**\n",
            "    *   **Data Representation:** All data, from images and text to sensor readings, is represented as vectors, matrices, and tensors in AI systems. Linear algebra provides the tools to manipulate these structures.\n",
            "    *   **Neural Networks:** Deep learning models are essentially complex chains of matrix multiplications and vector additions, followed by non-linear activations. Understanding linear algebra is critical to comprehending how neural networks process information.\n",
            "    *   **Dimensionality Reduction:** Techniques like PCA (Principal Component Analysis) use linear algebra to reduce the complexity of high-dimensional data, making it easier for agents to learn.\n",
            "    *   **Embeddings:** Representing words, concepts, or entities as dense vectors (embeddings) relies heavily on linear algebra.\n",
            "\n",
            "3.  **The Navigator of Optimization (Calculus):**\n",
            "    *   **Learning in Neural Networks:** The core mechanism by which neural networks \"learn\" is called backpropagation, which uses calculus (specifically, the chain rule for derivatives) to calculate how to adjust the model's weights to minimize prediction errors (loss functions).\n",
            "    *   **Optimization Algorithms:** Gradient descent and its variants (Adam, RMSprop, etc.) are based on finding the minimum of a function by following the direction of the steepest descent, calculated using derivatives.\n",
            "    *   **Understanding Model Behavior:** Calculus helps in understanding the sensitivity of models to input changes and the curvature of loss landscapes.\n",
            "\n",
            "4.  **The Architect of Logic and Structure (Discrete Mathematics):**\n",
            "    *   **Algorithms:** The design and analysis of algorithms â€“ the step-by-step instructions AI agents follow â€“ are deeply rooted in discrete math (set theory, graph theory, combinatorics).\n",
            "    *   **Logic and Reasoning:** Formal logic (propositional logic, predicate logic) is crucial for symbolic AI, knowledge representation, planning, and automated reasoning systems.\n",
            "    *   **Graph Theory:** Represents relationships between entities, essential for recommendation systems, social network analysis, pathfinding (e.g., A* search), and knowledge graphs.\n",
            "    *   **Computational Complexity:** Understanding how efficient an algorithm is (or isn't) relies on concepts from discrete math.\n",
            "\n",
            "5.  **Solving Real-World Problems (Numerical Methods & Optimization Theory):**\n",
            "    *   **Approximation:** Many real-world problems in AI don't have exact analytical solutions. Numerical methods provide techniques for finding approximate solutions efficiently (e.g., solving systems of equations, integration).\n",
            "    *   **Efficiency and Convergence:** Numerical stability, convergence rates, and error analysis are vital for building robust and practical AI systems.\n",
            "    *   **Optimal Decisions:** Optimization theory (a broader field encompassing calculus and linear algebra) provides frameworks for finding the best possible decisions or parameters given a set of constraints, crucial for reinforcement learning and planning.\n",
            "\n",
            "6.  **Understanding Space and Movement (Geometry & Control Theory):**\n",
            "    *   **Robotics and Computer Vision:** Geometric concepts (transformations, rotations, projections) are essential for robotic navigation, manipulating objects, and understanding 3D spaces in computer vision.\n",
            "    *   **Control Systems:** For AI agents that interact with the physical world (like robots or autonomous vehicles), control theory (which uses differential equations, linear algebra, and calculus) is critical for designing stable and effective controllers.\n",
            "\n",
            "In essence, mathematics provides the:\n",
            "*   **Theoretical Foundation:** To understand why AI algorithms work, not just how to use them.\n",
            "*   **Problem-Solving Tools:** To formulate AI problems in a way that computers can understand and solve.\n",
            "*   **Language for Innovation:** To read research papers, develop new algorithms, and push the boundaries of AI.\n",
            "\n",
            "Without a strong grasp of these mathematical disciplines, one is limited to being a user of existing AI tools rather than an innovator or a deep understander of the field.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Running Agent Asynchronously"
      ],
      "metadata": {
        "id": "NIv--hlPPAeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "    agent : Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"you only respond in haikus.\",\n",
        "    )\n",
        "result = await Runner.run(agent, \"Tell me about recursion is programming\", run_config=config)\n",
        "\n",
        "print(result.final_output)\n",
        "\n",
        "asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S87kpn3uKPHq",
        "outputId": "3698f4a2-2420-4526-94d2-1370e4d04374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's dive into recursion in programming.\n",
            "\n",
            "**What is Recursion?**\n",
            "\n",
            "At its core, recursion is a programming technique where a function calls itself within its own definition. Think of it like a set of Russian nesting dolls (Matryoshka dolls). Each doll contains a smaller version of itself. In recursion, each function call solves a smaller subproblem of the same type as the original problem.\n",
            "\n",
            "**Key Concepts**\n",
            "\n",
            "1.  **Base Case:**  Every recursive function *must* have a base case (or stopping condition). This is the condition that, when met, causes the function to stop calling itself and return a value directly. Without a base case, the function would call itself infinitely, leading to a stack overflow error (crashing the program).\n",
            "\n",
            "2.  **Recursive Step (or Recursive Call):** This is where the function calls itself with a modified version of the input. The input is modified in a way that moves it closer to the base case.  It breaks the problem down into smaller, self-similar subproblems.\n",
            "\n",
            "**How Recursion Works (Illustrative Example)**\n",
            "\n",
            "Let's consider a classic example: calculating the factorial of a number (n!).  The factorial of `n` is defined as `n * (n-1) * (n-2) * ... * 1`.\n",
            "\n",
            "Here's how we can implement this recursively in Python:\n",
            "\n",
            "```python\n",
            "def factorial(n):\n",
            "  \"\"\"\n",
            "  Calculates the factorial of a non-negative integer.\n",
            "\n",
            "  Args:\n",
            "    n: The integer to calculate the factorial of.\n",
            "\n",
            "  Returns:\n",
            "    The factorial of n.\n",
            "  \"\"\"\n",
            "  # Base case: Factorial of 0 is 1\n",
            "  if n == 0:\n",
            "    return 1\n",
            "  else:\n",
            "    # Recursive step: n! = n * (n-1)!\n",
            "    return n * factorial(n - 1)\n",
            "\n",
            "# Example usage\n",
            "result = factorial(5)  # Calculate 5!\n",
            "print(result)  # Output: 120\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1.  **`factorial(n)` Function:**  This function takes an integer `n` as input.\n",
            "\n",
            "2.  **Base Case:**  `if n == 0:`  If `n` is 0, the function returns 1. This is our stopping condition because the factorial of 0 is defined as 1.\n",
            "\n",
            "3.  **Recursive Step:**  `else: return n * factorial(n - 1)`  If `n` is not 0, the function returns `n` multiplied by the factorial of `n-1`.  This is the recursive call.  The function calls itself with a smaller value (`n-1`), gradually moving towards the base case.\n",
            "\n",
            "**Execution Flow (for `factorial(5)`):**\n",
            "\n",
            "1.  `factorial(5)` is called.\n",
            "2.  `5 != 0` is true, so it executes `return 5 * factorial(4)`\n",
            "3.  `factorial(4)` is called.\n",
            "4.  `4 != 0` is true, so it executes `return 4 * factorial(3)`\n",
            "5.  `factorial(3)` is called.\n",
            "6.  `3 != 0` is true, so it executes `return 3 * factorial(2)`\n",
            "7.  `factorial(2)` is called.\n",
            "8.  `2 != 0` is true, so it executes `return 2 * factorial(1)`\n",
            "9.  `factorial(1)` is called.\n",
            "10. `1 != 0` is true, so it executes `return 1 * factorial(0)`\n",
            "11. `factorial(0)` is called.\n",
            "12. `0 == 0` is true, so it returns `1` (base case).\n",
            "13. The results are then returned back up the call stack:\n",
            "    *   `factorial(1)` returns `1 * 1 = 1`\n",
            "    *   `factorial(2)` returns `2 * 1 = 2`\n",
            "    *   `factorial(3)` returns `3 * 2 = 6`\n",
            "    *   `factorial(4)` returns `4 * 6 = 24`\n",
            "    *   `factorial(5)` returns `5 * 24 = 120`\n",
            "\n",
            "**Advantages of Recursion:**\n",
            "\n",
            "*   **Elegance and Readability:**  For certain problems, recursion can lead to very clean and concise code that closely mirrors the mathematical or logical definition of the problem.  This can make the code easier to understand (once you grasp the concept of recursion).\n",
            "*   **Natural Fit for Certain Problems:**  Problems that can be naturally broken down into smaller, self-similar subproblems are often well-suited for recursive solutions. Examples include:\n",
            "    *   Tree traversals (e.g., traversing a directory structure)\n",
            "    *   Graph algorithms (e.g., depth-first search)\n",
            "    *   Divide-and-conquer algorithms (e.g., merge sort, quicksort)\n",
            "*   **Abstraction:** Recursion can hide the underlying complexity of the algorithm, making the code easier to reason about at a higher level.\n",
            "\n",
            "**Disadvantages of Recursion:**\n",
            "\n",
            "*   **Overhead:** Each recursive call adds overhead in terms of function call stack management. This can make recursive solutions slower and more memory-intensive than iterative solutions (using loops).\n",
            "*   **Stack Overflow:**  If the recursion goes too deep (i.e., the base case is not reached or the problem is too large), it can lead to a stack overflow error.  The call stack has a limited size.\n",
            "*   **Debugging:**  Debugging recursive functions can be more challenging than debugging iterative functions because you need to keep track of the call stack.\n",
            "\n",
            "**When to Use Recursion**\n",
            "\n",
            "*   **When the problem has a natural recursive structure.**  If the problem can be easily broken down into smaller, self-similar subproblems, recursion might be a good choice.\n",
            "*   **When readability is a priority.**  If a recursive solution makes the code cleaner and easier to understand, it might be worth the potential performance overhead.\n",
            "*   **When the depth of recursion is limited.**  If you know that the recursion will not go too deep, the risk of a stack overflow is reduced.\n",
            "\n",
            "**When to Avoid Recursion**\n",
            "\n",
            "*   **When performance is critical.**  If performance is a major concern, an iterative solution is often more efficient.\n",
            "*   **When the recursion depth is unknown or potentially very large.**  In such cases, an iterative solution is safer to avoid stack overflow errors.\n",
            "*   **When the problem is easily solved iteratively.** If an iterative solution is just as clear and concise as a recursive solution, it's usually the better choice.\n",
            "\n",
            "**Tail Recursion (and Tail Call Optimization)**\n",
            "\n",
            "*   **Tail Recursion:** A recursive function is tail-recursive if the recursive call is the *very last* operation performed in the function.  In other words, the function doesn't do any further calculations or processing after the recursive call returns.\n",
            "\n",
            "*   **Tail Call Optimization (TCO):** Some compilers and interpreters can optimize tail-recursive calls by reusing the current stack frame instead of creating a new one.  This effectively turns the recursion into iteration, eliminating the risk of stack overflow.  *However, Python does NOT implement tail call optimization.* This means that even if your function is tail-recursive, it will still consume stack space with each call. Some other languages like Scheme and certain functional languages do support TCO.\n",
            "\n",
            "**Example of Tail Recursion (in a language that supports TCO, conceptually):**\n",
            "\n",
            "```\n",
            "def tail_recursive_factorial(n, accumulator=1):\n",
            "  \"\"\"\n",
            "  Tail-recursive factorial calculation.\n",
            "\n",
            "  Args:\n",
            "    n: The integer to calculate the factorial of.\n",
            "    accumulator: An accumulator variable to store the intermediate result.\n",
            "\n",
            "  Returns:\n",
            "    The factorial of n.\n",
            "  \"\"\"\n",
            "  if n == 0:\n",
            "    return accumulator\n",
            "  else:\n",
            "    return tail_recursive_factorial(n - 1, n * accumulator)\n",
            "```\n",
            "\n",
            "In this example, the recursive call `tail_recursive_factorial(n - 1, n * accumulator)` is the very last thing that happens in the `else` block.  The result of the recursive call is immediately returned.\n",
            "\n",
            "**Iterative vs. Recursive**\n",
            "\n",
            "Any problem that can be solved recursively can also be solved iteratively (using loops), and vice versa.  The choice between recursion and iteration depends on the specific problem, performance considerations, and code clarity.\n",
            "\n",
            "**In Summary**\n",
            "\n",
            "Recursion is a powerful programming technique that can be used to solve a wide range of problems. However, it's important to understand its advantages and disadvantages and to use it judiciously.  Always make sure your recursive functions have a clear base case to prevent infinite recursion and potential stack overflows. And be aware of the potential performance overhead compared to iterative solutions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set_tracing_disabled(disabled=True)\n",
        "\n",
        "# # Client Setup for Connecting to Gemini\n",
        "# external_client:AsyncOpenAI = AsyncOpenAI(\n",
        "#     api_key=gemini_api_key,\n",
        "#     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "# )\n",
        "\n",
        "# #Initialize model\n",
        "# model:OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(\n",
        "#     model=\"gemini-2.5-flash\",\n",
        "#     openai_client=external_client\n",
        "# )\n",
        "\n",
        "# def main():\n",
        "#   # Create the Recipe Agent\n",
        "#   agent = Agent(\n",
        "#       name=\"RecipeBot\",\n",
        "#       instructions=(\n",
        "#           \"\"\"You are a helpful recipe assistant. A user will give you a few ingredients\n",
        "#           they have at home, and you will suggest one simple and quick recipe using only those items.\n",
        "#           Keep it short, step-by-step, and easy for beginners to cook.\"\"\"\n",
        "#       ),\n",
        "#       model=model\n",
        "#   )\n",
        "\n",
        "#   print(\"\\nðŸ³ What can I cook today?\\n\")\n",
        "#   ingredients = \"eggs, tomatoes, onions, and bread\"\n",
        "#   result:Runner = Runner.run_sync(agent, f\"I have these at home: {ingredients}. What can I cook?\")\n",
        "\n",
        "#   print(result.final_output)"
      ],
      "metadata": {
        "id": "H_gsdkokNkDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "locally run file \"chatbot.py\" in vs code"
      ],
      "metadata": {
        "id": "ygYmHltYYviT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chainlit as cl\n",
        "\n",
        "#from agents import Agent, RunConfig, AsyncOpenAI, OpenAIChatCompletionModel, Runner\n",
        "from agents import Agent, RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, Runner\n",
        "# from openai.types.chat import ResponseTextDeltaEvent\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "from agents.tool import function_tool\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "#step 1: Provider\n",
        "provider = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "#step 2: Model\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=provider,\n",
        ")\n",
        "\n",
        "\n",
        "#config: Define at run level\n",
        "run_config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=provider,\n",
        "    tracing_disabled=True,\n",
        ")\n",
        "\n",
        "#function tool call\n",
        "@function_tool(\"get_weather\")\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"\n",
        "    fatches the weather for a given location\n",
        "    \"\"\"\n",
        "    # In a real implementation, you would fetch weather data from an API.\n",
        "    # Here, we return a mock response for demonstration purposes.\n",
        "    return f\"The current weather in {location} is sunny with a temperature of 25Â°C.\"\n",
        "\n",
        " #Step 3: Agent\n",
        "agent1 = Agent(\n",
        "    instructions=\"you are a helpful assistant that can answer questions about general topics. use get_weather tool to share get temperature of any location.\",\n",
        "    name=\"Learning Agent\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "# #step 4: run\n",
        "# result = Runner.run_sync(\n",
        "#     input=\"What is the capital of France?\",\n",
        "#     run_config=run_config,\n",
        "#     starting_agent=agent1,\n",
        "# )\n",
        "\n",
        "# #step 4: run\n",
        "# result = Runner.run_sync(\n",
        "#     agent1,\n",
        "#     input=\"who is the founder of pakistan?\",\n",
        "#     run_config=run_config,\n",
        "# )\n",
        "\n",
        "# print(result.final_output)\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def handle_chat_start():\n",
        "    cl.user_session.set(\"history\", [])\n",
        "    await cl.Message(content=\"Hello! I'm the hyper colab support Agent. how i can hepl you.\").send()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @cl.on_message\n",
        "# async def handle_message(message: cl.Message):\n",
        "\n",
        "#     #history k reply k ly\n",
        "#     history = cl.user_session.get(\"history\")\n",
        "\n",
        "#     #streamiing k ly\n",
        "#     msg = cl.Message(content=\"\")\n",
        "#     await msg.send()\n",
        "\n",
        "\n",
        "#     #standard  interface [{\"role\": \"user\", \"content\": \"Hello!\"}, {\"role\": \"assistant\", \"content\": \"Hi there!\"\n",
        "#     history.append({\"role\": \"user\", \"content\": message.content})\n",
        "\n",
        "\n",
        "#     # result = await Runner.run(\n",
        "#     #     input=message.content,\n",
        "#     #     run_config=run_config,\n",
        "#     #     starting_agent=agent1,\n",
        "#     # )\n",
        "\n",
        "#     #straeming k ly await ko remove kara ga     result = await Runner.run(\n",
        "\n",
        "#     # result = await Runner.run(\n",
        "#     result= Runner.run_streamed(\n",
        "#         agent1,\n",
        "#         input=history,\n",
        "#         run_config=run_config,\n",
        "#     )\n",
        "\n",
        "#     #streaming k ly\n",
        "#     async for event in result.stream_events():\n",
        "#         if event.type == \"raw _response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "#             await msg.stream_token(event.data.delta)\n",
        "\n",
        "#     history.append({\"role\": \"assistant\", \"content\": result.final_output})\n",
        "#     cl.user_session.set(\"history\", history)\n",
        "#    #await cl.Message(content=result.final_output).send()\n",
        "#-------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# # At the top, remove these problematic imports:\n",
        "# # from openai.types.chat import ResponseTextDeltaEvent\n",
        "# # from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "# # In your handle_message function:\n",
        "# @cl.on_message\n",
        "# async def handle_message(message: cl.Message):\n",
        "#     history = cl.user_session.get(\"history\")\n",
        "\n",
        "#     msg = cl.Message(content=\"\")\n",
        "#     await msg.send()\n",
        "\n",
        "#     history.append({\"role\": \"user\", \"content\": message.content})\n",
        "\n",
        "#     # Use run_streamed correctly\n",
        "#     result = Runner.run_streamed(\n",
        "#         agent1,\n",
        "#         input=history,\n",
        "#         run_config=run_config,\n",
        "#     )\n",
        "\n",
        "#     # Stream the response - use stream_events() not output_stream()\n",
        "#     async for event in result.stream_events():\n",
        "#         # Check for text delta events and stream them\n",
        "#         if hasattr(event, 'data') and hasattr(event.data, 'delta'):\n",
        "#             await msg.stream_token(event.data.delta)\n",
        "\n",
        "#     # After streaming completes, get the final output\n",
        "#     final_result = await result  # Await to get the final RunResult\n",
        "\n",
        "#     await msg.update()  # Finalize the message\n",
        "\n",
        "#     history.append({\"role\": \"assistant\", \"content\": final_result.final_output})\n",
        "#     cl.user_session.set(\"history\", history)\n",
        "\n",
        "#   #-------------------------------------------------------------------------------------------------------\n",
        "@cl.on_message\n",
        "async def handle_message(message: cl.Message):\n",
        "    history = cl.user_session.get(\"history\")\n",
        "\n",
        "    msg = cl.Message(content=\"\")\n",
        "    await msg.send()\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": message.content})\n",
        "\n",
        "    # Use run_streamed correctly\n",
        "    result = Runner.run_streamed(\n",
        "        agent1,\n",
        "        input=history,\n",
        "        run_config=run_config,\n",
        "    )\n",
        "\n",
        "    # Stream the response\n",
        "    async for event in result.stream_events():\n",
        "        if hasattr(event, 'data') and hasattr(event.data, 'delta'):\n",
        "            await msg.stream_token(event.data.delta)\n",
        "\n",
        "    # Finalize the message\n",
        "    await msg.update()\n",
        "\n",
        "    # Get final output from the RunResultStreaming object directly\n",
        "    # (no await needed - it's already available after streaming completes)\n",
        "    history.append({\"role\": \"assistant\", \"content\": result.final_output})\n",
        "    cl.user_session.set(\"history\", history)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hnOeJxAWY44S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}